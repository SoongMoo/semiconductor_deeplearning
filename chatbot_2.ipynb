{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6992d673-4034-4e34-be85-003f4e4b0539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask\n",
      "  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting blinker>=1.9.0 (from flask)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting click>=8.1.3 (from flask)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting itsdangerous>=2.2.0 (from flask)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from flask) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from flask) (3.0.2)\n",
      "Collecting werkzeug>=3.1.0 (from flask)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Downloading flask-3.1.2-py3-none-any.whl (103 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Installing collected packages: werkzeug, itsdangerous, click, blinker, flask\n",
      "\n",
      "   ---------------------------------------- 0/5 [werkzeug]\n",
      "   ---------------------------------------- 0/5 [werkzeug]\n",
      "   ---------------------------------------- 0/5 [werkzeug]\n",
      "   ---------------------------------------- 0/5 [werkzeug]\n",
      "   ---------------------------------------- 0/5 [werkzeug]\n",
      "   ---------------------------------------- 0/5 [werkzeug]\n",
      "   ---------------------------------------- 0/5 [werkzeug]\n",
      "   -------- ------------------------------- 1/5 [itsdangerous]\n",
      "   ---------------- ----------------------- 2/5 [click]\n",
      "   ---------------- ----------------------- 2/5 [click]\n",
      "   ---------------- ----------------------- 2/5 [click]\n",
      "   ------------------------ --------------- 3/5 [blinker]\n",
      "   -------------------------------- ------- 4/5 [flask]\n",
      "   -------------------------------- ------- 4/5 [flask]\n",
      "   -------------------------------- ------- 4/5 [flask]\n",
      "   -------------------------------- ------- 4/5 [flask]\n",
      "   ---------------------------------------- 5/5 [flask]\n",
      "\n",
      "Successfully installed blinker-1.9.0 click-8.2.1 flask-3.1.2 itsdangerous-2.2.0 werkzeug-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09570b-71e1-4f29-b234-297decdfba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template_string, request\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# 모델 준비\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "chat_history = \"\"\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    global chat_history\n",
    "    bot_reply = \"\"\n",
    "    if request.method == \"POST\":\n",
    "        user_input = request.form[\"message\"]\n",
    "        chat_history += f\"User: {user_input}\\nBot:\"\n",
    "        inputs = tokenizer(chat_history, return_tensors=\"pt\").to(device)\n",
    "        # 업데이트된 대화 기록(chat_history)을 AI 모델이 이해할 수 있는 숫자 형태(텐서)로 변환하고, \n",
    "        # 이를 모델 연산이 수행될 장치(GPU 또는 CPU)로 이동시킴\n",
    "        # 변환된 토큰 시퀀스를 PyTorch 텐서(Tensor) 형태로 반환하도록 지정합니다. (pt는 PyTorch를 의미합니다.)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            # max_new_tokens를 사용하여 새로 생성될 봇 응답의 최대 길이를 100~200 정도로 설정합니다.\n",
    "            max_new_tokens=100, \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        bot_reply = response.split(\"Bot:\")[-1].split(\"User:\")[0].strip()\n",
    "        chat_history += f\" {bot_reply}\\n\"\n",
    "\n",
    "    return render_template_string(\"\"\"\n",
    "        <h2>한국어 GPT2 챗봇</h2>\n",
    "        <form method=\"post\">\n",
    "            <input name=\"message\" placeholder=\"메시지를 입력하세요\" style=\"width:300px\">\n",
    "            <input type=\"submit\" value=\"Send\">\n",
    "        </form>\n",
    "        <p><b>Bot:</b> {{bot_reply}}</p>\n",
    "    \"\"\", bot_reply=bot_reply)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
