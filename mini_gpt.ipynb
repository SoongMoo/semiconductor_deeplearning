{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61306b-d68c-45e9-905e-664c43657813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini Generative Pre-trained Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258a18b-684e-4478-82b9-f58179d0fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# 샘플 문장 데이터\n",
    "# ----------------------------\n",
    "sentences = [\n",
    "    \"안녕하세요 오늘 날씨가 좋네요\",\n",
    "    \"저는 학생입니다\",\n",
    "    \"오늘은 텐서플로우를 공부합니다\",\n",
    "    \"자연어 처리는 재미있습니다\",\n",
    "    \"문장을 생성하는 모델을 만들어봅시다\"\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# 단어 토크나이저\n",
    "# ----------------------------\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', lower=False, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # 0번 패딩 포함\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_seq_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "# 시퀀스 패딩\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_seq_len, padding='post')\n",
    "y_train = np.roll(X_train, shift=-1, axis=1)  # 다음 단어 예측\n",
    "y_train[:, -1] = 0  # 마지막 토큰은 0으로 채우기\n",
    "\n",
    "# ----------------------------\n",
    "# Positional Encoding\n",
    "# ----------------------------\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_len, embed_dim):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = self.positional_encoding(max_len, embed_dim)\n",
    "\n",
    "    def positional_encoding(self, max_len, embed_dim):\n",
    "        positions = np.arange(max_len)[:, np.newaxis]\n",
    "        dims = np.arange(embed_dim)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (dims // 2)) / np.float32(embed_dim))\n",
    "        angle_rads = positions * angle_rates\n",
    "        pos_encoding = np.zeros((max_len, embed_dim))\n",
    "        pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        return tf.cast(pos_encoding[np.newaxis, ...], tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
    "\n",
    "# ----------------------------\n",
    "# Decoder Block (동적 causal mask)\n",
    "# ----------------------------\n",
    "class DecoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.norm1 = layers.LayerNormalization()\n",
    "        self.norm2 = layers.LayerNormalization()\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        # 입력 길이에 맞는 causal mask 생성\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        mask = mask[tf.newaxis, tf.newaxis, :, :]  # (1,1,seq_len,seq_len)\n",
    "        attn_output = self.attn(x, x, attention_mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.norm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.norm2(out1 + ffn_output)\n",
    "\n",
    "# ----------------------------\n",
    "# 모델 생성\n",
    "# ----------------------------\n",
    "embed_dim = 32\n",
    "num_heads = 2\n",
    "ff_dim = 64\n",
    "num_layers = 2\n",
    "\n",
    "inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = PositionalEncoding(max_seq_len, embed_dim)(x)\n",
    "\n",
    "for _ in range(num_layers):\n",
    "    x = DecoderBlock(embed_dim, num_heads, ff_dim)(x, training=True)\n",
    "\n",
    "outputs = layers.Dense(vocab_size)(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "model.summary()\n",
    "\n",
    "# ----------------------------\n",
    "# 학습\n",
    "# ----------------------------\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=4, verbose=0)\n",
    "\n",
    "# ----------------------------\n",
    "# 문장 생성 함수\n",
    "# ----------------------------\n",
    "def generate_sentence(model, start_words, max_len=10):\n",
    "    seq = tokenizer.texts_to_sequences([start_words])[0]\n",
    "    for _ in range(max_len):\n",
    "        padded_seq = tf.expand_dims(seq, 0)  # 배치 차원 추가\n",
    "        logits = model(padded_seq, training=False)  # mask는 DecoderBlock에서 자동 생성\n",
    "        next_token = tf.argmax(logits[:, -1, :], axis=-1)[0].numpy()\n",
    "        if next_token == 0:\n",
    "            break\n",
    "        seq.append(next_token)\n",
    "    return ' '.join(tokenizer.index_word[i] for i in seq)\n",
    "\n",
    "# ----------------------------\n",
    "# 문장 생성 예시\n",
    "# ----------------------------\n",
    "start_words = \"오늘은\"\n",
    "generated_sentence = generate_sentence(model, start_words)\n",
    "print(\"Generated:\", generated_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086897c4-3df4-448a-b5ce-37fb6ed1ab50",
   "metadata": {},
   "source": [
    "### 제대로 된 GPT를 만들려면 \n",
    "1. 모델 규모 및 파라미터 수\n",
    "GPT 모델은, 비록 \"미니\" 버전일지라도 수백만, 수십억 개의 파라미터를 가집니다. 하지만 이 코드는 레이어 수가 적고 embed_dim (32)이 매우 작아 파라미터 수가 극히 적다. \n",
    "\n",
    "2. 학습 데이터\n",
    "GPT 모델은 책, 기사, 웹사이트 등 인터넷의 방대하고 다양한 텍스트 코퍼스로 학습하게 한다.\n",
    "현 데이터로는 모델이 의미 있는 언어 지식을 학습하기에 턱없이 부족하며 결과적으로 모델은 제공된 시퀀스를 \"암기\"하는 수준에 머무르며, 매우 제한적이고 반복적인 출력만을 생성할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
